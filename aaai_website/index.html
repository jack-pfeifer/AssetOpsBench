<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>From Inception to Productization: Hands-on Lab for the Lifecycle of Multimodal Agentic AI in Industry 4.0</title>
  <link rel="stylesheet" href="style.css"/>
</head>
<body>
<header>
  <h1>From Inception to Productization:</h1>
  <h1>Hands-on Lab for the Lifecycle of Multimodal Agentic AI in Industry 4.0</h1>

  <div class="logos">
    <!-- Left logos -->
    <div class="left-logos">
      <img src="figs/aaai-logo-RGB.jpg" alt="AAAI Logo" width="150" height="80"/>
      <img src="figs/ibm.png" alt="IBM Logo" width="220" height="80"/>
    </div>

    <!-- Center buttons -->
    <div class="center-buttons">
      <a href="https://github.com/IBM/AssetOpsBench" target="_blank">AssetOpsBench GitHub</a>
      <a href="https://huggingface.co/spaces/ibm-research/AssetOps-Bench" target="_blank">AssetOpsBench Hugging Face Playground</a>
        <a href="https://huggingface.co/spaces/ibm-research/AssetOps-Bench" target="_blank">AssetOpsBench Blog</a>
      <a href="https://huggingface.co/datasets/ibm-research/AssetOpsBench" target="_blank">Datasets</a>
            <a href="Resources/AAAI_2026_Lab_Slides.pdf" target="_blank">Lab Slides</a>

    </div>

    <!-- Right logos -->
    <div class="right-logos">
      <img src="figs/AIISC_logo.png" alt="AIISC Logo" width="150" height="80"/>
      <img src="figs/USClogo.jpg" alt="USC Logo" width="220" height="80"/>
    </div>
  </div>

  <p class="event-info">January 2026, 4:15pm-6:00pm SST | AAAI-26 | Singapore EXPO, Room Opal 104</p>
</header>


<nav>
  <ul>
    <li><a href="#abstract">Abstract</a></li>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#goal">Goals of the Lab</a></li>
    <li><a href="#schedule">Schedule</a></li>
    <li><a href="#technical-setup">Technical Setup</a></li>
    <li><a href="#resources">Supplementary Materials</a></li>
    <li><a href="#references">References</a></li>
    <li><a href="#biographies">Organizers</a></li>
      <li><a href="#prep">Pre-Lab Setup</a></li>

  </ul>
</nav>

<section id="abstract">
  <h2>Abstract</h2>
  <p>
    Agentic AI is poised to transform decision-making in Industry 4.0 by enabling autonomous agents to reason over multimodal inputs—such as sensor streams, structured knowledge bases, and unstructured maintenance logs—and act adaptively under uncertainty. Yet, real-world adoption remains challenging due to data fragmentation, integration complexity, limited explainability, and lack of evaluation workflows.
  </p>
  <p>
    This hands-on tutorial offers a full lifecycle walkthrough for building trustworthy agentic AI systems in industrial settings. Participants will engage in two interactive labs: (i) resolving data silos in smart manufacturing using an open-source platform, and (ii) benchmarking agent performance, reasoning, and explainability in an enterprise-scale industrial simulation. Capabilities such as trace visualizations, real-time introspection, and comparative reasoning will be demonstrated. The session concludes with best practices for governance, monitoring, and reusable evaluation workflows. Participants will leave with practical skills and modular tools to build explainable, robust, and deployable agentic AI systems for real-world Industry 4.0 applications.
  </p>
</section>

<section id="introduction">
  <h2>Introduction</h2>
  <p>
    Agentic AI is rapidly becoming a cornerstone of intelligent decision-making in Industry 4.0. These agents must reason across heterogeneous data sources—including sensor time series, structured knowledge graphs, and unstructured logs—while adapting under uncertainty. Despite advances in large language models and multimodal learning, building deployable and trustworthy systems remains difficult due to fragmented data, lack of explainability, and limited evaluation protocols.
  </p>
  <p>
    This tutorial provides a comprehensive, hands-on walkthrough of the lifecycle of multimodal agentic AI—from design to deployment—featuring lab sessions on data integration and benchmarking. We explore reasoning strategies, evaluation methods, and governance tools that ensure trustworthy and auditable deployments.
  </p>
</section>

<section id="goal">
  <h2>Goals of the Lab</h2>
  <ul>
    <li>Understand the full lifecycle of multimodal agentic AI systems in Industry 4.0.</li>
    <li>Learn key agent architectures and reasoning strategies, including Plan-Execute, ReAct, Reflexion, RAFA, and hybrid symbolic-neural methods.</li>
    <li>Gain hands-on experience integrating multimodal data (sensors, text, knowledge graphs).</li>
    <li>Diagnose integration challenges such as data silos and misaligned modalities.</li>
    <li>Explore evaluation methods using human feedback, sensor-grounded truth, and LLM-as-a-judge approaches.</li>
    <li>Apply governance and monitoring techniques for agent traceability, explainability, and coordination.</li>
  </ul>
</section>

<section id="schedule">
  <h2>Schedule</h2>
  <table>
    <caption>Table 1: Detailed Outline of the Lab</caption>
    <thead>
      <tr><th>Time</th><th>Activity</th><th>Presenter(s)</th></tr>
    </thead>
    <tbody>
      <tr><td>10 mins</td><td>Introduction and Overview: Multimodal AI agents, use cases in Industry 4.0, objectives.</td><td>Amit Sheth, Dhaval Patel</td></tr>
      <tr><td>20 mins</td><td>Multimodal Agents in Industry 4.0: Overview of architectures (symbolic + neural integration).</td><td>Ruwan Wickramarachchi</td></tr>
      <tr><td>20 mins</td><td>Lab Session 1: Addressing Data Silos and Integration Complexity.</td><td>Chathurangi Shyalika</td></tr>
      <tr><td>20 mins</td><td>Operationalizing and Governing Multimodal Agents: Evaluation and governance techniques.</td><td>Dhaval Patel, Saumya Ahuja</td></tr>
      <tr><td>20 mins</td><td>Lab Session 2: Evaluation Benchmarking at Scale for Industrial Multi-Agent Systems.</td><td>Shuxin Lin</td></tr>
      <tr><td>15 mins</td><td>Q&amp;A and Wrap-up</td><td>All Presenters</td></tr>
    </tbody>
  </table>
</section>

<section id="technical-setup">
  <h2>Technical Setup</h2>
  <p>Participants should bring a laptop with Python 3 installed. Pre-configured environments and setup instructions will be provided. The tutorial uses:</p>
  <ul>
    <li>Python 3</li>
    <li>Google Colab or Jupyter Notebooks</li>
    <li>Hugging Face for LLM-driven agents</li>
    <li>LLM access for agent execution</li>
    <li>Simulation environments: <a href="https://huggingface.co/datasets/ibm-research/AssetOpsBench">AssetOpsBench</a>, SmartPilot, FailureSensorIQ, FutureFactories dataset</li>
    <li>Optional Docker setup for isolated local execution</li>
  </ul>
</section>

<section id="prep" class="prep-section">
  <h2>Get Ready for the AAAI-2026 Agent Lab</h2>
  <p class="prep-intro">
    To make sure you’re ready, please complete these steps before the lab:
  </p>

  <div class="prep-steps">
    <!-- Step 1 -->
    <article class="prep-card">
      <div class="prep-step-number">1</div>
      <h3>Join the Codabench Competition (AAAI Lab Session)</h3>
      <p>
        We have set up a dedicated AAAI Lab Session competition on Codabench.
      </p>
      <ul>
        <li>Go to: <a href="https://www.codabench.org/competitions/10206/" target="_blank">https://www.codabench.org/competitions/10206/</a></li>
        <li>Click on any tab so the platform prompts you to join the challenge.</li>
        <li>We have enabled auto-enrollment, so you should then be added automatically.</li>
      </ul>
      <p class="prep-note">
        <strong>Important:</strong> Simply creating a Codabench account is not enough — you must explicitly join this competition.
      </p>
    </article>

    <!-- Step 2 -->
    <article class="prep-card">
      <div class="prep-step-number">2</div>
      <h3>Complete the Setup up to Step 3</h3>
      <p>
        Please follow the setup instructions here up to Step 3:
      </p>
      <p>
        <a href="https://github.com/IBM/AssetOpsBench/blob/main/benchmark/cods_track1/README_CODS.md" target="_blank">
          AssetOpsBench Setup Instructions
        </a>
      </p>
      <p>
        Having this environment ready will allow you to focus on the agent design and experimentation during the session rather than basic setup.
      </p>
    </article>

    <!-- Step 3 -->
    <article class="prep-card">
      <div class="prep-step-number">3</div>
      <h3>⭐ Give a GitHub Star to the Benchmark Repository</h3>
      <p>
        Please star ⭐ our GitHub repository:
      </p>
      <p>
        <a href="https://github.com/IBM/AssetOpsBench" target="_blank">https://github.com/IBM/AssetOpsBench</a>
      </p>
      <p>
        This helps you stay notified of any updates we push before the lab and also signals community interest, which supports continued maintenance and improvements.
      </p>
    </article>
  </div>
</section>


<section id="resources">
  <h2>Supplementary Materials</h2>
  <ol>
    <li><a href="https://www.youtube.com/watch?v=USsZkobb8lI">AAAI-2025 Lab Video</a></li>
    <li><a href="https://github.com/IBM/anomaly-detection-code-pattern">Anomaly Detection Code Pattern</a></li>
    <li><a href="https://aimx.global/aimx-2024/">Trustworthy AI @ SWITCH AIMX 2024</a></li>
    <li><a href="https://www.ibm.com/events/reg/flow/ibm/Q62V7PMB/landing/page/landing">Multi-Agent Systems for Enterprise: TechXchange 2025</a></li>
    <li><a href="https://huggingface.co/datasets/ibm-research/AssetOpsBench">AssetOpsBench</a></li>
    <li><a href="https://www.codabench.org/competitions/10206/">AssetOpsBench Challenge @ CODS-COMAD 2025</a></li>
    <li><a href="https://github.com/ChathurangiShyalika/SmartPilot">SmartPilot CoPilot System</a></li>
    <li><a href="https://github.com/IBM/FailureSensorIQ">FailureSensorIQ Dataset</a></li>
    <li><a href="https://wiki.aiisc.ai/index.php?title=Smart_manufacturing">Smart Manufacturing Wiki</a></li>
    <li><a href="https://wiki.aiisc.ai/index.php?title=Neurosymbolic_Artificial_Intelligence_Research_at_AIISC">Neurosymbolic AI Wiki</a></li>
  </ol>
</section>

<section id="references">
  <h2>References</h2>
  <ul>
    <li>Patel et al. 2025. AssetOpsBench: Benchmarking AI Agents for Task Automation. <a href="https://arxiv.org/pdf/2506.03828">arXiv:2506.03828</a></li>
    <li>Constantinides et al. 2025. FailureSensorIQ: A Multi-Choice QA Dataset. <a href="https://arxiv.org/pdf/2506.03278">arXiv:2506.03278</a></li>
    <li>Shyalika et al. 2025. SmartPilot: Multiagent CoPilot for Manufacturing. <a href="https://arxiv.org/pdf/2505.06492">arXiv:2505.06492</a></li>
    <li>Harik et al. 2024. FutureFactories Dataset. <a href="https://arxiv.org/pdf/2401.15544">arXiv:2401.15544</a></li>
    <li>Patel et al. 2024. AI Model Factory: Scaling AI for Industry 4.0. <i>AAAI Proceedings</i></li>
  </ul>
</section>

<section id="biographies">
  <h2>Organizers</h2>

  <div class="biography">
    <img src="figs/Chathurangi_Shyalika.jpeg" alt="Chathurangi Shyalika"  width="150" height="150"/>
    <h3><a href="https://www.linkedin.com/in/chathurangi-shyalika-1b89229b/" target="_blank">Chathurangi Shyalika</a></h3>
    <p>Ph.D. student at AIISC, University of South Carolina. Research in Deep Learning, Multimodal-AI, Neurosymbolic-AI, anomaly detection, event understanding.</p>
  </div>

  <div class="biography">
    <img src="figs/saumya_ahuja.jpeg" alt="Saumya Ahuja" width="150" height="150"/>
    <h3><a href="https://www.linkedin.com/in/saumyahuja/" target="_blank">Saumya Ahuja</a></h3>
    <p>AI Engineer Lead, IBM WatsonX ASEAN. Leads Generative AI and Agentic AI projects across APAC. Experienced in LLMs, RAG systems, and enterprise AI deployments.</p>
  </div>

  <div class="biography">
    <img src="figs/shuxin_lin.jpeg" alt="Shuxin Lin" width="150" height="150"/>
    <h3><a href="https://www.linkedin.com/in/shuxin-lin/" target="_blank">Shuxin Lin</a></h3>
    <p>Researcher at IBM with expertise in AI for Industry 4.0, agent evaluation, multimodal reasoning, and large-scale industrial AI benchmarks.</p>
  </div>

  <div class="biography">
    <img src="figs/Ruwan_Wickramarachchi.png" alt="Ruwan Wickramarachchi" width="150" height="150"/>
    <h3><a href="https://ruwantw.github.io/" target="_blank">Ruwan Wickramarachchi</a></h3>
    <p>Research Scientist at Bosch Center for AI. Ph.D. from AIISC, USC. Research in Generative AI, Neurosymbolic AI, knowledge graphs, and multimodal representation learning.</p>
  </div>

  <div class="biography">
    <img src="figs/Dhaval_Patel.png" alt="Dhaval Patel" width="150" height="150"/>
    <h3><a href="https://www.linkedin.com/in/dhaval-patel-2b287033/" target="_blank">Dhaval Patel</a></h3>
    <p>Senior Technical Staff Member, IBM Research. Expert in Data Mining, Machine Learning, Time Series, and industrial AI platforms such as Maximo and AutoAI-TS.</p>
  </div>

  <div class="biography">
    <img src="figs/Amit_Sheth.png" alt="Amit Sheth" width="150" height="150"/>
    <h3><a href="https://amit.aiisc.ai/" target="_blank">Amit Sheth</a></h3>
    <p>NCR Chair & Professor, AIISC, USC. Fellow of IEEE, AAAI, ACM, AAAS. Research in trustworthy, explainable, and safe neuro-symbolic AI.</p>
  </div>

</section>
</body>
</html>
